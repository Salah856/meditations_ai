{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Testing\n",
    "\n",
    "* [YouTube | Generative Adversial Network (GANs) Full Coding Example Tutorial in Tensorflow 2.0!](https://www.youtube.com/watch?v=tX-6CMNnT64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import data as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 64\n",
    "SEQLEN = 100\n",
    "BUFFER = 10000\n",
    "EMBED_DIM = 256\n",
    "EPOCHS = 10\n",
    "LATENT_UNITS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 507 stoic lessons from Marcus Aurelius\n",
      "From my grandfather Verus I learned good morals and the government of my temper.\n",
      "From the reputation and remembrance of my father, modesty and a manly character.\n",
      "From my mother, piety and beneficence,\n"
     ]
    }
   ],
   "source": [
    "# import The Meditations by Marcus Aurelius\n",
    "txt = d.meditations()\n",
    "print(txt[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(txt):\n",
    "    vocab = sorted(set(txt))  # create vocab from text string (txt)\n",
    "    char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "    data_idx = np.array([char2idx[c] for c in txt])\n",
    "    \n",
    "    # max length sequence we can have\n",
    "    max_seqlen = len(txt) // (SEQLEN)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_idx)\n",
    "    \n",
    "    sequences = dataset.batch(SEQLEN + 1, drop_remainder=True)\n",
    "    \n",
    "    dataset = sequences.shuffle(BUFFER).batch(BATCHSIZE, drop_remainder=True)\n",
    "    \n",
    "    return dataset, char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, char2idx = format_data(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Useful Links:\n",
    "* [Medium | Music generation with Neural Networks](https://medium.com/cindicator/music-generation-with-neural-networks-gan-of-the-week-b66d01e28200)\n",
    "* [arXiv | C-RNN-GAN: Continuous recurrent neural networks with adversial training](https://arxiv.org/pdf/1611.09904.pdf)\n",
    "* [Medium | Generating Pokemon-Inspired Musiic from Neural Networks](https://towardsdatascience.com/generating-pokemon-inspired-music-from-neural-networks-bc240014132)\n",
    "\n",
    "For the loss functions, we will use categorical cross entropy for both as both LSTM and Generative models can use it. This article [Understanding Categorical Cross-Entropy Loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/) covers this type of loss well.\n",
    "\n",
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_units, vocab_size, seqlen, batchsize):\n",
    "    # dense -> leakyrelu ->  batchnorm * 3 -> reshape to seqlen\n",
    "    # final dense must be len(vocab_size) which we will then map to chars\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # dense -> leakyrelu -> batchnorm\n",
    "    model.add(tf.keras.layers.Dense(256, input_shape=(latent_units,)))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    # dense -> leakyrelu -> batchnorm\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    # dense -> leakyrelu -> batchnorm\n",
    "    model.add(tf.keras.layers.Dense(1024))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    # reshape output to seqlen\n",
    "    model.add(tf.keras.layers.Dense(vocab_size*seqlen, activation='softmax'))\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 256)               256256    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 6700)              6867500   \n",
      "=================================================================\n",
      "Total params: 7,787,820\n",
      "Trainable params: 7,784,236\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_generator = build_generator(LATENT_UNITS, len(char2idx), SEQLEN, BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(vocab_size, embed_dim, batchsize, units):\n",
    "    model = tf.keras.Sequential()\n",
    "    # add our embedding layer\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embed_dim,\n",
    "                                        batch_input_shape=[batchsize, None]))\n",
    "    # the LSTM layer\n",
    "    model.add(tf.keras.layers.LSTM(units, return_sequences=True,\n",
    "                                   stateful=True, dropout=.1))\n",
    "    # a DNN layer\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    # leaky ReLU activation layer\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    # final binary classifier layer, 1 or 0\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "  \n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           17152     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 67)            34371     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (64, None, 67)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (64, None, 1)             68        \n",
      "=================================================================\n",
      "Total params: 1,626,503\n",
      "Trainable params: 1,626,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_discriminator = build_discriminator(len(char2idx), EMBED_DIM, BATCHSIZE, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
